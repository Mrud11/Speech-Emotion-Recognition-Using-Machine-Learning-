# Speech-Emotion-Recognition-Using-Machine-Learning-

Analyzing emotions based on audio measures using machine learning objective and classify the corresponding emotion. There are many models that can be used for sound based recognition projects such as speech, music, songs, etc. But Here I am determining the emotional state of humans. It is an idiosyncratic task and may be used as a standard for any emotion recognition model. Among the numerous models used for categorization of these emotions, a discrete emotional approach is considered as one of the fundamental approaches. It uses various emotions such as anger, boredom, disgust, surprise, fear, joy, happiness, neutral and sadness. Deep Neural Networks (DNNs) are based on feed-forward structures comprised of one or more underlying hidden layers between inputs and outputs. The feed-forward architectures such as Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs) provide efficient results for image and video processing. 
 Speech Emotion Recognition is a technology that extracts emotion features from computer speech signals, and analyzes the feature parameters and the obtained emotion changes. Recognizing emotions from audio signals requires feature extraction and classifier training. The feature vector is composed of audio signal elements that characterize the specific characteristics of speaker (such as pitch, volume, energy) which is essential for training the classifier model to accurately recognize specific emotions. Recurrent architectures such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) are much effective in speech-based classification such as natural language processing (NLP) and SER. Apart from their effective way of classification these models do have some limitations. For instance, the positive aspect of CNN’s is to learn features from high-dimensional input data, and also learns features from small variations and distortion occurrence and hence, requires large storage capability. Similarly, LSTM-based RNNs are able to handle variable input data and model long-range sequential text data. 
DATA COLLECTION

 	The first step is Data collection. This step is very important because the quality and quantity of the data gathered will directly affect the level of the prediction model.       This data is taken from Kaggle website which contains the different voice recordings of the persons.

 DATASET DESCRIPTION:

	There are a set of 200 target words were spoken in the carrier phrase "Say the word _' by two actresses (aged 26 and 64 years) and recordings were made of the set portraying each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). There are 2800 data points (audio files) in total.
The dataset is organised such that each of the two female actor and their emotions are contain within its own folder. And within that, all 200 target words audio file can be found. The format of the audio file is a WAV format.

ATTRIBUTES

	Anger

	Disgust

	Fear

	Happiness

	Pleasant surprise

	Sadness

	Neutral
